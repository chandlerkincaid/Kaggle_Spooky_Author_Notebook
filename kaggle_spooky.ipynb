{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Spooky Author Challenge\n",
    "### In this notebook I use the [Kaggle Spooky Author Dataset](https://www.kaggle.com/c/spooky-author-identification) to demonstrate some natural language processing techniques in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Pandas](https://pandas.pydata.org/) Dataframe is a useful data structure for this project and comes with some easy ways to read in CSV files. We will use [NLTK](http://www.nltk.org/) to demonstrate some fundamental NLP processes. NLTK needs to download some components separate from its install but fortunately we should only need to do this once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      text author\n",
      "id                                                               \n",
      "id26305  This process, however, afforded me no means of...    EAP\n",
      "id17569  It never once occurred to me that the fumbling...    HPL\n",
      "id11008  In his left hand was a gold snuff box, from wh...    EAP\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv', index_col='id')\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only using the training dataset for this demonstration. The testing dataset does not have labels as it is meant to be submitted to the online grader. The data comes with a built in index column so we might as well use it to keep our data organized. These are examples of the raw sentences from the data. There are three targets in the form of authors Edgar Allan Poe, H.P Lovecraft, and Marry Shelly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stripping Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "id26305    This process however afforded me no means of a...\n",
      "id17569    It never once occurred to me that the fumbling...\n",
      "id11008    In his left hand was a gold snuff box from whi...\n",
      "Name: stripped, dtype: object\n"
     ]
    }
   ],
   "source": [
    "example = data\n",
    "example['stripped'] = example['text'].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "print(example[:3]['stripped'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to make a copy of the data to play around with and you will see why in a moment. When using Dataframes we can apply a function to each row element in a particular column by using the \"apply\" function and a lambda expression. The regular expression \"re.sub(r'[^\\w\\s]','',x)\" strips punctuation off of the sentences which is a critical aspect to creating uniform features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "id26305    [This, process, however, afforded, me, no, mea...\n",
      "id17569    [It, never, once, occurred, to, me, that, the,...\n",
      "id11008    [In, his, left, hand, was, a, gold, snuff, box...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "example['tokens'] = example['stripped'].apply(lambda x: nltk.word_tokenize(x))\n",
    "print(example[:3]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This expression simply breaks sentences into individual words. These words will become the building blocks of our feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "id26305    [this, process, however, afforded, me, no, mea...\n",
      "id17569    [it, never, once, occurred, to, me, that, the,...\n",
      "id11008    [in, his, left, hand, was, a, gold, snuff, box...\n",
      "Name: lower, dtype: object\n"
     ]
    }
   ],
   "source": [
    "example['lower'] = example['tokens'].apply(lambda x: [word.lower() for word in x])\n",
    "print(example[:3]['lower'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming words to lowercase is a common practice in NLP. The primary reasoning for this is that a word at the start of a sentence should be an equivalent feature to the same word occurring later in a sentence, all other things being equal. There may be specific situations where you do not want to do this but it is common for most text classification problems. In this function each row element \"x\" is no longer a single string but a list due to the tokenization process. For this reason we need a list comprehension to iterate over each list to transform each item to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "id26305    [process, however, afforded, means, ascertaini...\n",
      "id17569    [never, occurred, fumbling, might, mere, mistake]\n",
      "id11008    [left, hand, gold, snuff, box, capered, hill, ...\n",
      "Name: stopped, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "example['stopped'] = example['lower'].apply(lambda x: [word for word in x if word not in stopWords])\n",
    "print(example[:3]['stopped'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are generally very common words that tend to not vary in frequency between different classes. Some examples are \"as\", \"if\", \"is\", \"the\" \"a\". For most text classification problems removing stopwords is an easy way to cut down on features that do not convey much information and improve the performance of classifiers down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "id26305    [process, howev, afford, mean, ascertain, dime...\n",
      "id17569           [never, occur, fumbl, might, mere, mistak]\n",
      "id11008    [left, hand, gold, snuff, box, caper, hill, cu...\n",
      "Name: stemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "example['stemmed'] = example['stopped'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "print(example[:3]['stemmed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a common process in NLP which removes certain affixes to get to the core of a word and reduce the size of the feature space. You can see how some of these words have been altered such as \"afforded\" to \"afford\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All in one with Scikit-Learn Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aback', 'abandon', 'abaout', 'abas', 'abash', 'abat', 'abbey', 'abbrevi', 'abdic', 'abdul']\n"
     ]
    }
   ],
   "source": [
    "def stem_func(text):\n",
    "    return [stemmer.stem(word) for word in analyzer(text)]\n",
    "\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=stopWords,\n",
    "    analyzer=stem_func,\n",
    "    ngram_range=(1,3),\n",
    "    min_df=2\n",
    ")\n",
    "x_data = vectorizer.fit_transform(data['text'])\n",
    "print(vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have demonstrated each of these operations individually with NLTK to get an idea of what they look like, their flow, and their purpose. Wouldn't it be nice if we could combine these operations into a single efficient command? It turns out we can with Scikit-Learn CountVectorizer. In addition to stripping punctuation, creating tokens, transforming to lowercase, removing stopwords, and stemming, CountVectorizer can also create a range of N-grams and count these patterns. These counts are used to form a numeric feature vector that we can use for machine learning. \n",
    "\n",
    "Using an N-gram range of one to three will capture most patterns in the text that are common enough to make decent features for this problem. We use a minimum document frequency of two to ensure words appear at least twice in the corpus to be considered a feature. To incorporate the stemming functionality from NLTK we have to write a short function to override the built in analyzer. As a sanity check we'll print the first ten features to get an idea of what they look like. \n",
    "\n",
    "CountVectorizer has a convenient version called TfidfVectorizer which outputs term frequency inverse document frequency instead of raw counts. This can be a nice addition for some classifiers but our best classifier works better with raw counts so we'll stick to that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels to numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'MWS']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_data = le.fit_transform(data['author'])\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have transformed our text into numeric feature vectors but we also need to convert our target labels into numeric values so that they can be used by our classifiers. LabelEncoder will take care of that. Print the classes LabelEncoder creates is helpful for ensuring that we do not have any messy labels and our data is clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test, Train Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.33, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make a training and testing set to evaluate our classifiers. Fortunately, Scikit-Learn has a convenient one liner to do just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "selector = SelectPercentile(score_func=chi2, percentile=90)\n",
    "x_train = selector.fit_transform(x_train, y_train)\n",
    "x_test = selector.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature space is already productive but we can gain a small performance improvement by discard the 10% worst features according to the [chi2 metric](https://en.wikipedia.org/wiki/Chi-squared_distribution). We transformed the training and testing x sets to make sure the dimensionality remains consistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "names = [\"GaussianNB\", \"MultinomialNB\", \"BernoulliNB\"]\n",
    "\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem we are going to examine three flavors of [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier). In this context, Naive Bayes is a simple classifier that looks at the probability of a pattern occurring in a document, relative to the prior probability of a category and the evidence the classifier has trained on. For text classification Naive Bayes is fast and often performs better than more complicated algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def metrics(name, real, pred):\n",
    "    print(name)\n",
    "    acc = accuracy_score(real, pred)\n",
    "    f1 = f1_score(real, pred, average='weighted')\n",
    "    print(\"ACC: \" + str(acc))\n",
    "    print(\"F1: \" + str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use some built in scoring functions from Scikit-Learn to analyze classifier performance. [F1](https://en.wikipedia.org/wiki/F1_score) is a great metric which balances precision and recall. Since this data set has three labels its a good choice to use a \"weighted\" setting which weights the score based on the prevalence for each category label. A high F1 score derived in this way will reflect that our classifier is not too specific or too general for any one category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB\n",
      "ACC: 0.6935933147632312\n",
      "F1: 0.6931428017154011\n",
      "MultinomialNB\n",
      "ACC: 0.8308573197152584\n",
      "F1: 0.8308631638197698\n",
      "BernoulliNB\n",
      "ACC: 0.8245125348189415\n",
      "F1: 0.8242845741505971\n"
     ]
    }
   ],
   "source": [
    "for name, classifier, in zip(names, classifiers):\n",
    "    classifier.fit(x_train.toarray(), y_train)\n",
    "    predictions = classifier.predict(x_test.toarray())\n",
    "    metrics(name, y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop goes through our classifiers to train, test, and evaluate their performance. 83% is a respectable outcome for a three category NLP classification task. Next time we will examine some advanced NLP techniques such as different variants of stemming vs lemmatization, wrapper based feature selection, and other classifiers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
